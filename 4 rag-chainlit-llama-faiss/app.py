from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain import PromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import CTransformers
import chainlit as cl

# Path to the FAISS database used for vector storage
DB_FAISS_PATH = "vectorstore/db_faiss"

# Template for custom prompts used in querying the model. This template guides the model to provide
# answers based on the given context and question. It instructs the model to admit when it does not
# know an answer, ensuring responses are informative and honest.
custom_prompt_template = """

Use the following pieces of information to answer the user's question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context: {context}
Question: {question}

Only return the helpful answer below and nothing else.
Helpful answer:

"""

# Prompt template for QA retrieval for each vectorstore
def set_custom_prompt():    
    """
    Defines a function to set a custom prompt template for question-answering (QA) retrieval.
    
    This function creates a new PromptTemplate object with a predefined template and input variables.
    The template is designed for QA interactions, specifying how the context and question should be
    presented to the model for generating answers. The 'custom_prompt_template' variable used here
    should be defined elsewhere in the code, containing the actual template string.
    
    Returns:
        A PromptTemplate object configured for QA retrieval, using the specified template and input variables.
    """
    prompt = PromptTemplate(
        template=custom_prompt_template, input_variables=["context", "question"]
    )
    return prompt


# Retrieval QA Chain
def retrieval_qa_chain(llm, prompt, db):
    """
    Creates a retrieval-based QA chain using a language model, a prompt, and a database retriever.
    
    Args:
        llm: The language model to be used for generating answers.
        prompt: The prompt template to guide the model's responses.
        db: The database object used to retrieve relevant documents based on the query.
    
    Returns:
        A configured RetrievalQA object that combines document retrieval with language model-based answering.
    """
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # Placeholder for the actual chain type. Should be replaced with the correct type.
        retriever=db.as_retriever(search_kwargs={"k": 2}),  # Configures the retriever with 'k' nearest documents.
        return_source_documents=True,  # Indicates whether to return the documents used for generating the answer.
        chain_type_kwargs={"prompt": prompt},  # Passes the custom prompt to the chain.
    )
    return qa_chain


# Loading the model
def load_llm():
    """
    Loads a pre-trained language model for use in generating responses.
    
    Returns:
        An instance of the CTransformers class configured with the specified model and settings.
    """
    # Load the locally downloaded model here
    llm = CTransformers(
        model="TheBloke/Llama-2-7B-Chat-GGML",  # Model identifier.
        model_type="llama",  # Type of the model.
        max_new_tokens=512,  # Maximum number of new tokens to generate.
        temperature=0.5,  # Sampling temperature.
    )
    return llm


# QA Model Function
def qa_bot():
    """
    Initializes the QA bot by loading the necessary components: embeddings, database, and language model.
    
    Returns:
        A configured QA chain that can be used to process queries and generate responses.
    """
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",  # Embedding model identifier.
        model_kwargs={"device": "cpu"},  # Forces the model to run on CPU.
    )
    db = FAISS.load_local(DB_FAISS_PATH, embeddings)  # Loads the FAISS database with the specified embeddings.
    llm = load_llm()  # Loads the language model.
    qa_prompt = set_custom_prompt()  # Sets the custom prompt template.
    qa = retrieval_qa_chain(llm, qa_prompt, db)  # Initializes the QA chain.

    return qa


# output function
def final_result(query):
    """
    Processes a query through the QA bot and returns the response.
    
    Args:
        query: The user's query as a string.
    
    Returns:
        The response generated by the QA bot.
    """
    qa_result = qa_bot()  # Initializes the QA bot.
    response = qa_result({"query": query})  # Processes the query and gets the response.
    return response


# chainlit code
@cl.on_chat_start
async def start():
    chain = qa_bot()
    msg = cl.Message(content="Starting the bot...")
    await msg.send()
    msg.content = "Hi, Welcome to the Camino Santiago Bot. What is your query?"
    await msg.update()

    cl.user_session.set("chain", chain)


@cl.on_message
async def main(message):
    chain = cl.user_session.get("chain")
    cb = cl.AsyncLangchainCallbackHandler(
        stream_final_answer=True, answer_prefix_tokens=["FINAL", "ANSWER"]
    )
    cb.answer_reached = True
    res = await chain.acall(message, callbacks=[cb])
    answer = res["result"]
    sources = res["source_documents"]

    if sources:
        answer += f"\nSources:" + str(sources)
    else:
        answer += "\nNo sources found"

    await cl.Message(content=answer).send()
